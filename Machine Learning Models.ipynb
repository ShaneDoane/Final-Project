{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e9503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install ipython-sql sqlalchemy psycopg2 notebook pandas -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931f84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e33bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c10567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import os\n",
    "import psycopg2\n",
    "from config import host, user, password, database\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfb039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbbebeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teams',\n",
       " 'all_game_results',\n",
       " 'conferences',\n",
       " 'rankings_with_team_names',\n",
       " 'conferences_with_team_names',\n",
       " 'sag_system',\n",
       " 'rankings']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# from sqlalchemy import extract\n",
    "# connection_string = f\"postgresql://{user}:{password}@{host}/{database}\"\n",
    "# engine = create_engine(connection_string)\n",
    "# from sqlalchemy import inspect\n",
    "# insp = inspect(engine)\n",
    "# insp.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdd8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base = automap_base()\n",
    "# Base.prepare(engine,reflect=True)\n",
    "# Teams= Base.classes.teams\n",
    "# #Games= Base.classes.all_game_results\n",
    "# Conf= Base.classes.conferences\n",
    "# #Ranks= Base.classes.rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f77259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = Session(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35483b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# teams_results = session.query(Teams).all()\n",
    "# len(teams_results)\n",
    "\n",
    "# # teams_df = session.query(Teams)\n",
    "# # teams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5bbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query_to_dict(rset):\n",
    "#     result = defaultdict(list)\n",
    "#     for obj in rset:\n",
    "#         instance = inspect(obj)\n",
    "#         for key, x in instance.attrs.items():\n",
    "#             result[key].append(x.value)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e225278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teamsdf = pd.DataFrame(query_to_dict(teams_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940e6966",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'teams' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19316/53120759.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteams_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteams_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19316/53120759.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteams_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteams_results\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'teams' object is not iterable"
     ]
    }
   ],
   "source": [
    "# teams_list = [list(x) for x in teams_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30b54459",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teams_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19316/1219759783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteams_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteams_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mteams_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teams_list' is not defined"
     ]
    }
   ],
   "source": [
    "teams_df = pd.DataFrame(teams_list)\n",
    "teams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_game_path = 'Resources/all_game_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7fb33af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_game_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19316/428815990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read in CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mNew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_game_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mNew_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_game_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Read in CSV\n",
    "New_df = pd.read_csv(all_game_path)\n",
    "New_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64293474",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_all_game_df = New_df.drop(columns='Unnamed: 0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_all_game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season_2014_df = dropped_all_game_df.loc[dropped_all_game_df['Season']== 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season_2014_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f19597",
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_df = dropped_all_game_df[dropped_all_game_df['TeamID']==1103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c046f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e65fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "team2_df = dropped_all_game_df[dropped_all_game_df['TeamID']==1102]\n",
    "team2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4397c73",
   "metadata": {},
   "source": [
    "# Random balanced Fores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features\n",
    "team1 = team1_df.drop(columns='Outcome')\n",
    "team1_dummies_df = pd.get_dummies(team1)\n",
    "\n",
    "\n",
    "# Create our target\n",
    "team1_outcome_df = team1_df['Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c17c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balance of our target values\n",
    "print(team1_outcome_df.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(team1_dummies_df, \n",
    "                                                    team1_outcome_df, \n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with the BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "rf_model = BalancedRandomForestClassifier(n_estimators=100,random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e227f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated the balanced accuracy score\n",
    "y_pred = rf_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "balanced_accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03522a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26394137",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(zip(rf_model.feature_importances_, team1_dummies_df.columns), reverse=True)\n",
    "for importance in importances:\n",
    "    print(f\"{importance[1]}: ({importance[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67030435",
   "metadata": {},
   "source": [
    "# Team 2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "team2 = team2_df.drop(columns='Outcome')\n",
    "team2_dummies_df = pd.get_dummies(team2)\n",
    "team2_outcome_df = team2_df['Outcome']\n",
    "print(team2_outcome_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(team2_dummies_df, \n",
    "                                                    team2_outcome_df, \n",
    "                                                    random_state=1)\n",
    "print(X_train2.shape)\n",
    "print(Counter(y_train2))\n",
    "print(Counter(y_test2))\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c124d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with the BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "rf_model2 = BalancedRandomForestClassifier(n_estimators=100,random_state=1)\n",
    "rf_model2.fit(X_train2, y_train2)\n",
    "Counter(y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = rf_model2.predict(X_test2)\n",
    "from sklearn.metrics import accuracy_score\n",
    "balanced_accuracy_score(y_test2,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba70f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test2, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131148ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(zip(rf_model.feature_importances_, team2_dummies_df.columns), reverse=True)\n",
    "for importance in importances:\n",
    "    print(f\"{importance[1]}: ({importance[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987100dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937093aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199d16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2env",
   "language": "python",
   "name": "ml2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
